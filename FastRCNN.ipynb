{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Private Dataset\n",
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from bs4 import BeautifulSoup\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import matplotlib.patches as patches\n",
    "import os\n",
    "import cv2\n",
    "from skimage.filters import threshold_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_image(img_tensor, annotation):\n",
    "    \n",
    "    \n",
    "    fig,ax = plt.subplots(1)\n",
    "    \n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    \n",
    "    img = img_tensor.cpu().data\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(img.permute(1, 2, 0))\n",
    "    \n",
    "    for box in annotation[\"boxes\"]:\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "\n",
    "        # Create a Rectangle patch\n",
    "        rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
    "\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_image_normal(img, annotation):\n",
    "    \n",
    "    \n",
    "    fig,ax = plt.subplots(1)\n",
    "    \n",
    "    fig.set_size_inches(50, 50)\n",
    "    \n",
    "    # Display the image\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    for box in annotation[\"boxes\"]:\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "\n",
    "        # Create a Rectangle patch\n",
    "        rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
    "\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def opencv_resize(image, ratio):\n",
    "    width = int(image.shape[1] * ratio)\n",
    "    height = int(image.shape[0] * ratio)\n",
    "    dim = (width, height)\n",
    "    return cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "def plot_rgb(image):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    return plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "def plot_gray(image):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    return plt.imshow(image, cmap='Greys_r')\n",
    "\n",
    "def generate_box(obj):\n",
    "    \n",
    "    xmin = int(obj.find('xmin').text)\n",
    "    ymin = int(obj.find('ymin').text)\n",
    "    xmax = int(obj.find('xmax').text)\n",
    "    ymax = int(obj.find('ymax').text)\n",
    "    \n",
    "    return [xmin, ymin, xmax, ymax]\n",
    "\n",
    "def generate_label(obj):\n",
    "    if obj.find('name').text == \"solar\":\n",
    "        return 1\n",
    "\n",
    "def generate_target(image_id, file): \n",
    "    with open(file) as f:\n",
    "        data = f.read()\n",
    "        soup = BeautifulSoup(data, 'xml')\n",
    "        objects = soup.find_all('object')\n",
    "\n",
    "        num_objs = len(objects)\n",
    "\n",
    "        # Bounding boxes for objects\n",
    "        # In coco format, bbox = [xmin, ymin, width, height]\n",
    "        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for i in objects:\n",
    "            boxes.append(generate_box(i))\n",
    "            labels.append(generate_label(i))\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # Labels (In my case, I only one class: target class or background)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        # Tensorise img_id\n",
    "        img_id = torch.tensor([image_id])\n",
    "        # Annotation is in dictionary format\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = img_id\n",
    "        \n",
    "        return target"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8697c1a15901ab7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "imgs_short_path = list(sorted(os.listdir(\"/kaggle/input/solar-panels-detection/images/\")))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "710f80c276948fbd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels_short_path = list(sorted(os.listdir(\"/kaggle/input/solar-panels-detection/annotations/xmls/\")))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ecf99903056741"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MaskDataset(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(\"/kaggle/input/solar-panels-detection/images/\")))\n",
    "#         self.labels = list(sorted(os.listdir(\"/kaggle/input/face-mask-detection/annotations/\")))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        file_image =  imgs_short_path[idx] \n",
    "        file_label =  labels_short_path[idx]\n",
    "        img_path = os.path.join(\"/kaggle/input/solar-panels-detection/images/\", file_image)\n",
    "        label_path = os.path.join(\"/kaggle/input/solar-panels-detection/annotations/xmls/\", file_label)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        #Generate Label\n",
    "        target = generate_target(idx, label_path)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a02aa8ba0ac8e579"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "        transforms.ToTensor(), \n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cb90c7590489ca3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "dataset = MaskDataset(data_transform)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=4, collate_fn=collate_fn)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "677a8c7c0f9387d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18c85e2995bedd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9ec17a4848f1db"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = get_model_instance_segmentation(3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b80ed19574c4b858"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "for imgs, annotations in data_loader:\n",
    "    imgs = list(img.to(device) for img in imgs)\n",
    "    annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "    print(annotations)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41f89ddb4bf491d8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange,tqdm\n",
    "num_epochs = 20\n",
    "model.to(device)\n",
    "    \n",
    "# parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "len_dataloader = len(data_loader)\n",
    "epoch = 0\n",
    "for epoch in trange(num_epochs, desc = 'Epoch #' + str(epoch)):\n",
    "    model.train()\n",
    "    i = 0    \n",
    "    epoch_loss = 0\n",
    "    for imgs, annotations in data_loader:\n",
    "        i += 1\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "        loss_dict = model([imgs[0]], [annotations[0]])\n",
    "        losses = sum(loss for loss in loss_dict.values())        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step() \n",
    "#         if i%25 == 0 :\n",
    "#             print(f'Iteration: {i}/{len_dataloader}, Loss: {losses}')\n",
    "        epoch_loss += losses\n",
    "#     print('Epoch : ',epoch,', Epoch Loss : ',epoch_loss.item() )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73e14d78f1bf5c1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for imgs, annotations in data_loader:\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd3c34b7ec15c2e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "preds = model(imgs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c62b763d8eef2728"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Prediction\")\n",
    "plot_image(imgs[2], preds[2])\n",
    "print(\"Target\")\n",
    "plot_image(imgs[2], annotations[2])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9817cae977089a85"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'model.pt')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b728fc69ccfc2a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model2 = get_model_instance_segmentation(3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98134d42ada2b337"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model2.load_state_dict(torch.load('model.pt'))\n",
    "model2.eval()\n",
    "model2.to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b8f18629cdfbf2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_image = Image.open('/kaggle/input/solar-panels/Screenshot 2022-12-24 115513.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9918408a7cf8a9a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np_sample_image = np.array(sample_image.convert(\"RGB\"))\n",
    "\n",
    "transformed_img = torchvision.transforms.transforms.ToTensor()(\n",
    "        torchvision.transforms.ToPILImage()(np_sample_image))\n",
    "\n",
    "result = model([transformed_img.to(torch.device('cuda'))])\n",
    "\n",
    "result"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7b74b22254b948d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_image(transformed_img, result[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a246f011f1eaeea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image = cv2.imread('/kaggle/input/solar-panels/Screenshot 2022-12-24 115513.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c440893c54746c9d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cropped_image = image * 0\n",
    "plt.figure(figsize=(20,20))\n",
    "for i in result[0]['boxes'].cpu().detach().numpy():\n",
    "    X,Y,W,H = int(i[0]),int(i[1]),int(i[2]), int(i[3])\n",
    "    cropped_image[Y:H, X:W] = image[Y:H, X:W]\n",
    "    plt.imshow(cropped_image)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "224c7140277c5aaf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert to grayscale for further processing\n",
    "gray = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2GRAY)\n",
    "plot_image_normal(gray, result[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31ef8688957492a8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "edged = cv2.Canny(gray, 100, 200, apertureSize=3)\n",
    "plot_image_normal(edged, result[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc5232fdcb26b7cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Detect all contours in Canny-edged image\n",
    "contours, hierarchy = cv2.findContours(edged, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "image_with_contours = cv2.drawContours(image.copy(), contours, -1, (0,255,0), 1)\n",
    "plot_image_normal(image_with_contours, result[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e97ce16706ce914c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for cnt in contours:\n",
    "    x1,y1 = cnt[0][0]\n",
    "    approx = cv2.approxPolyDP(cnt, 0.01*cv2.arcLength(cnt, True), True)\n",
    "    if len(approx) == 4:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        img = cv2.drawContours(cropped_image, [cnt], -1, (0,255,255), 3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19987988d2ba66c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_image_normal(img, result[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa0e4f4a6bacda78"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
